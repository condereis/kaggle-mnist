{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Digit Recognizer\n",
    "\n",
    "# Model Engineering\n",
    "\n",
    "## 1 - Introduction\n",
    "\n",
    "> The goal in this competition is to take an image of a handwritten single digit, and determine what that digit is.  As the competition progresses, we will release tutorials which explain different machine learning algorithms and help you to get started.\n",
    "\n",
    "> The data for this competition were taken from the MNIST dataset. The MNIST (\"Modified National Institute of Standards and Technology\") dataset is a classic within the Machine Learning community that has been extensively studied.  More detail about the dataset, including Machine Learning algorithms that have been tried on it and their levels of success, can be found at http://yann.lecun.com/exdb/mnist/index.html.\n",
    "\n",
    "\n",
    "## 2 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33600, 794)\n",
      "(8399, 794)\n",
      "(28000, 784)\n"
     ]
    }
   ],
   "source": [
    "project_dir = os.path.join(os.path.dirname('__file__'), os.pardir)\n",
    "settings = json.loads(open(os.path.join(project_dir, 'SETTINGS.json')).read())\n",
    "train_path = os.path.join(project_dir, settings['TRAIN_DATA_PATH'])\n",
    "test_path = os.path.join(project_dir, settings['TEST_DATA_PATH'])\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "train_imgs = train_data.drop('label', axis=1)\n",
    "one_hot_target = pd.get_dummies(train_data['label'], prefix='dig')\n",
    "train_data = pd.concat([train_imgs, one_hot_target], axis=1, join='inner')\n",
    "\n",
    "train_val_ratio = 0.8\n",
    "train_data_size = len(train_data)\n",
    "train_set = train_data[:int(train_data_size*train_val_ratio)]\n",
    "val_set = train_data[int(train_data_size*train_val_ratio)+1:]\n",
    "\n",
    "print(train_set.shape)\n",
    "print(val_set.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As discribed on Kaggle's website:\n",
    "\n",
    "> Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
    "\n",
    "> The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "> Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n",
    "\n",
    "\n",
    "The image bellow shows a sample of data from train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(10, 10)\n",
    "for row in range(10):\n",
    "    for column in range(10):\n",
    "        entry = train_data[train_data['label']==column].iloc[row].drop('label').as_matrix()\n",
    "        axarr[row, column].imshow(entry.reshape([28, 28]))\n",
    "        axarr[row, column].get_xaxis().set_visible(False)\n",
    "        axarr[row, column].get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Logistic Regression\n",
    "\n",
    "The first model, that will serve as a benchmark, will be [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression). We use [tensorflow](https://www.tensorflow.org) in order to get a well suited model.\n",
    "\n",
    "\n",
    "### 3.1 - Model\n",
    "\n",
    "First we will define the placeholder for input x. Then we define the weights and biases of our model as Variables. Finally we will define our Logist Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input tensor\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# weights (w) and biases (s) \n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# model output (y)\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 - Training\n",
    "\n",
    "First we will define the placeholder for the targets of our train data. Then the cross entropy can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# target\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "# cross entropy\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will describe bellow the train step and the accuracy mesure of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train teh model we first need to initialize all variables. Second we will iterate over many epochs and evaluate the model using a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "train_eval_list = []\n",
    "val_eval_list = []\n",
    "for i in range(1000):\n",
    "    batch = train_set.sample(frac=0.1)\n",
    "    batch_xs = batch.drop('label', axis=1).as_matrix()/255.0\n",
    "    batch_ys = pd.get_dummies(batch['label']).as_matrix()\n",
    "    val_xs = val_set.drop('label', axis=1).as_matrix()/255.0\n",
    "    val_ys = pd.get_dummies(val_set['label']).as_matrix()\n",
    "\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    " \n",
    "    train_eval = sess.run(accuracy, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    val_eval = sess.run(accuracy, feed_dict={x: val_xs, y_: val_ys})\n",
    "    \n",
    "    train_eval_list.append(train_eval)\n",
    "    val_eval_list.append(val_eval)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Evaluation\n",
    "\n",
    "As we can see the training did not overfit. So it is ready to be aplied to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(train_eval_list, label='Train set')\n",
    "plt.plot(val_eval_list, label='Validation set')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell runs the model on the test data and generates a CSV file for Kaggle's submission. This submission scores 0.91714. That is not a particularly good result for MNIST, where the state of art can score over 0.99. We will work on other models to try to get closer to this score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = sess.run(y, feed_dict={x: test_data.as_matrix() / 255.0})\n",
    "pred = [[i + 1, np.argmax(one_hot_list)] for i, one_hot_list in enumerate(predict)]\n",
    "submission = pd.DataFrame(pred, columns=['ImageId', 'Label'])\n",
    "submission_path = os.path.join(project_dir, settings['SUBMISSION_PATH'], 'logistic_regression.csv')\n",
    "submission.to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep this trained model for further use, we can just save the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(project_dir, settings['MODEL_PATH'], \"logistic_regression.ckpt\")\n",
    "saver.save(sess, model_path)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Deep Convolutional Neural Network\n",
    "\n",
    "To get closer to stat of art accuracy, let's try to apply a Convolutional Neural Network.\n",
    "\n",
    "### 4.1 - Model\n",
    "\n",
    "Let's define functions to create weight and bias variables in a way to avoid 0 gradients and \"dead neurons\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define our convolution and pooling operations, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our first convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "x = tf.placeholder(tf.float32, shape=[None,784])\n",
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the layer output by convolving the image with the weight, adding bias and max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do pretty much the same for the second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a fully connected layer as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define dropout to avoid overfiting and a softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError(\"Nesting violated for default stack of <type 'weakref'> objects\",) in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f8a80f2f8d0>> ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "step 0, training accuracy 0.04\n",
      "step 0, validation accuracy 0.1\n",
      "---\n",
      "step 100, training accuracy 0.56\n",
      "step 100, validation accuracy 0.67\n",
      "---\n",
      "step 200, training accuracy 0.78\n",
      "step 200, validation accuracy 0.79\n",
      "---\n",
      "step 300, training accuracy 0.86\n",
      "step 300, validation accuracy 0.87\n",
      "---\n",
      "step 400, training accuracy 0.76\n",
      "step 400, validation accuracy 0.87\n",
      "---\n",
      "step 500, training accuracy 0.88\n",
      "step 500, validation accuracy 0.92\n",
      "---\n",
      "step 600, training accuracy 0.92\n",
      "step 600, validation accuracy 0.94\n",
      "---\n",
      "step 700, training accuracy 0.94\n",
      "step 700, validation accuracy 0.89\n",
      "---\n",
      "step 800, training accuracy 0.9\n",
      "step 800, validation accuracy 0.92\n",
      "---\n",
      "step 900, training accuracy 0.94\n",
      "step 900, validation accuracy 0.91\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(1000):\n",
    "    batch = train_set.sample(50)\n",
    "    batch_xs = batch.filter(like='pixel',axis=1).as_matrix()/255.0\n",
    "    batch_ys = batch.filter(like='dig',axis=1).as_matrix()\n",
    "    if i%100 == 0:\n",
    "        val_batch = train_set.sample(500)\n",
    "        val_xs = val_batch.filter(like='pixel',axis=1).as_matrix()/255.0\n",
    "        val_ys = val_batch.filter(like='dig',axis=1).as_matrix()\n",
    "        train_eval = accuracy.eval(feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "        val_eval = accuracy.eval(feed_dict={x: val_xs, y_: val_ys, keep_prob: 0.5})\n",
    "        print('---')\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_eval))\n",
    "        print(\"step %d, validation accuracy %g\"%(i, val_eval))\n",
    "    train_step.run(feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b526cbb34c8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MODEL_PATH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"conv_nn.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'saver' is not defined"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(project_dir, settings['MODEL_PATH'], \"conv_nn.ckpt\")\n",
    "saver.save(sess, model_path)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
